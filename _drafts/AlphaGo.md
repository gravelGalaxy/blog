---
title: AlphaGo
tags: 大数据与机器学习
---

# 下围棋所需要进行的思考
1. 思考的广度b：有那些位置可以下。
2. 思考的深度d：我们下了某一布后棋局会怎样变化，对方会怎样下，我们又要接着怎样下。
最理想的情况是我们即考虑了深度，又考虑了广度，那么我们需要`b^d`次思考，但这个状态空间太大，是不合理的。
# AlphaGo所采用的策略
1. 思考的广度：使用策略分布`P(a|s)`，即在某个局面`s`下，可能走的位置`a`。通过这种方法我们能随机采样下一步走的位置（高概率的地方）。
2. 思考的深度：通过`棋局评估`来降低深度，通过一个`近似`的值来评估棋局的`好坏`，而不是去思考这步棋下完之后棋局的`演化`。通过值函数`V`，将当前的棋局`s`代入，就能知道这步棋下的好坏程度`V(s)`
# AlphaGo的总体结构
1. 监督学习的策略网络：使用人类专家的棋谱进行训练，目标是**下出和人类专家一样的棋**。
2. 快速走子网络：牺牲走棋的质量来获取更快的走棋时间，类似于人类下棋时的第一感受，有一定的参考意义。
3. 强化学习策略网络：以`监督学习的策略网络`为基础，不断和自己对弈，在监督学习的基础上调整优化目标（**赢棋**）。该过程也会产生新的数据集
4. 价值网络：用于从全局预测某个棋面下棋赢的概率。

最后需要使用蒙特卡洛搜索树组装在一起。
# AlphaGo所用的四种网络
## 监督学习策略网络(SL)
- 数据需要：大量人类专家棋谱。
- 构建标签：<棋面,人类专家下一步下棋的位置>，棋谱：有一个时间序列的下棋矩阵。
- 模型输入：围棋是一个`19 * 19`的矩阵，每一个位置有三种情况：
1. 白子，值为1
2. 黑子，值为-1
3. 无子，值为0

所以我们输入`19 * 19`的矩阵就能表达所有特征。

如果我们输入已知的先验知识，增加输入特征的维度，可以大大提高训练的效率。例如围棋的`气、目、空、打吃`。

如果我们的SL网络选择48中特征，那么我们的输入维度就是`19 * 19 * 48`
- 模型输出
在`19 * 19`的棋盘下各个位置下子的概率，取`最大`的那个作为预测的下棋位置。
- 准确率：使用13层卷积神经网络，达到`57%`准确率。
## 快速走子网络(SL)
- 目的：形成一种下棋的直觉。
- 输入：需要更少的输入特征。
- 下棋速度：很快
- 用途：在行棋推演时快速行棋。
- 正确率：24.2%， 速度：2微秒
## 强化学习策略网络(RL)
- 目的：之前都是考虑下的和专家的是否相同，根本没有考虑输赢。并且提供的数据实际上还是不够。为了提高算法的上限（提供样本），使用强化学习策略网络提供更多高质量的样本。
- 输出：某一棋面下下棋`动作`的`概率分布`。
- 结构：以监督学习网络(SL)为基础进行训练。
- 训练过程：
1. 随机选择前面某个迭代周期得到的SL网络，然后使用当前的RL策略网络和其对弈，目标是战胜选择的SL网络。随机的从一系列不同迭代周期的监督学习策略网络Pσ中选择对手是为了防止过拟合。
2. 在前面的思考中提过，我们之前完全没考虑到棋局的胜负。所以这里将棋局和棋局的胜负都考虑进来，用强化学习算法Poliicy Gradient以最大化赢的期望的目标更新参数。Policy Gradient算法简单来说就是如果在当前棋局下某步棋导致最后奖励变多（赢了），那就加大在这个棋局下下这步的概率，反之亦然，如果有读者感兴趣阅读我之前有关Policy Gradient的讲解文章。
3. 每隔500步就复制现在的参数进入我们的对手阵营中供第二步使用。

在这里规定赢棋奖励就是1，输棋就是-1，和棋或者没下完奖励就是0。在每局中有很多时间步t, 每个时间步对应一个t时刻的棋面St，不断使用前一轮训练的强化学习策略网络进行自我对弈，一直到时间步T决出了该局的胜负奖励r(ST)。此时进行神经网络参数更新，也就是回过头将该胜负标签r(ST)贴到该局前面每个时间步对应的棋面St的标签上。

- 实验结果：胜率`80%`
## 强化学习价值网络
- 输出：是否`赢棋`的预测值。
- 训练过程：
1. RL价值网络的训练很直接，就是直接使用一个棋局的胜负进行`回归训练`，以`最小化均方误差`为目标进行`随机梯度下降`。
2. 这里要注意，如果我们直接从人类完整棋局中学习价值网络会导致`过拟合`。因为不同的棋面之间存在很强的相关性，有的甚至只有一个落子不同，导致价值网络学习时直接记住最后的输赢，对同一对局而言`输入稍有不同`而`输出都相同`，而不能`泛化`到新的棋局。
3. 所以RL价值网络训练使用的三千万棋面来自三千万局棋，即`每局棋`我们只取其中`一个棋面`进行训练，有效防止过拟合。
4. RL价值网络的训练数据也不是单纯由强化学习自我对弈而来，在每局对弈的过程中先使用`SL网络`下子，然后`随机`下子，最后再使用`RL策略网络`下子，充分保障了数据的多样性。
- 实验结果：均方差0.22
# 蒙特卡洛搜索树
- 目的：解决`棋局推演`的问题。
- 核心要素：节点、边。每个节点是一个棋面s，每下完一步棋，就会变成下一个棋面，即当前节点的子节点。边是在当前节点s下的一步棋a，其中保留了动作的效益值，访问次数和先验概率。
- 步骤：其搜索模拟过程分为四步：选择Selection、扩展Expansion、评估Evaluation、反馈更新Backup
1. 选择。从根节点出发，选择在这个棋面下最好的下法。使用动作效益值函数评估。鼓励探索那些看起来还行（先验概率高）但没怎么尝试过的下法（访问次数低）
2. 扩展。当某个叶子节点的访问次数超过一定阈值后，进行扩展。
3. 评估。对上述叶子节点进行胜率的评估，判断当前棋面局势。方法有两种：通过价值网络预测胜率；通过快速走子策略，从叶子节点出发，快速行棋到最后，综合统计输赢来预测胜率。
4. 反馈更新。算法从根节点（当前棋面）选择访问次数最多的边对应的动作作为最终决策。

